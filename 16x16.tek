\documentclass{article}
\usepackage[inline]{enumitem}
\usepackage{baked}
\usepackage[left=1.5in, right=2in, top=0.45in, bottom=0.45in]{geometry}
\begin{document}
ECE-467, Deep Learning - Quiz $373866\pi$\begin{enumerate}
\item  How does the average attention distance change across layers in a Vision Transformer?
\item  What are some approximations that have been attempted in order to apply Transformers in the context of image processing?
\item  How do Sparse Transformers and specialized attention architectures help to scale attention for computer vision tasks?
\item  How important is dataset size when using a Vision Transformer?
\item  How does the Vision Transformer (ViT) differ from traditional convolutional networks?
\item  How does the Axial Transformer block work?
\item  How do convolutional networks and self-attention compare in computer vision tasks?
\item  What challenges have prevented the successful scaling of self-attention in large-scale image recognition tasks?
\item  How can a Vision Transformer be used to replace components of a convolutional network?
\item  How can a Vision Transformer be used to create an input sequence from a CNN feature map?
\item  How does the performance of Convolutional Neural Networks (CNNs) compare to Vision Transformers (ViT) when applied to image recognition tasks?
\item  How does training on large datasets affect the performance of CNNs and ViTs?
\item  What is the advantage of using a scalable NLP Transformer architecture for computer vision tasks? What are the efficient implementations of this architecture? How does this setup compare to state-of-the-art convolutional networks?
\item  What are the differences between theoretical FLOPs and inference speed on hardware?
\item  What challenges remain for Vision Transformer (ViT)?
\item  How do self-supervised pre-training methods affect the performance of ViT?
\item  How can a Vision Transformer (ViT) be used to perform well on image classification tasks?
\item  What are the advantages of using a Transformer over a convolutional network for computer vision tasks?
\item  How does the Transformer architecture integrate information across an entire image?
\item  What is the "attention distance" and how does it compare to receptive field size in CNNs?
\item  How does self-attention integrate information across an image?
\end{enumerate}
\end{document}
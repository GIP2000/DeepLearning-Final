\documentclass{article}
\usepackage[inline]{enumitem}
\usepackage{baked}
\usepackage[left=1.5in, right=2in, top=0.45in, bottom=0.45in]{geometry}
\begin{document}
ECE-467, Deep Learning - Quiz $326037\pi$\begin{enumerate}
\item  How does the Transformer model use embeddings to convert input and output tokens to vectors?
\item  How does the Transformer model differ from other sequence transduction models in terms of weight sharing between the two embedding layers and the pre-softmax linear transformation?
\item  What is the Transformer network architecture, and how does it improve upon existing models for machine translation tasks?
\item  How does the Transformer model compare to other models in terms of performance, training costs, and parallelizability?
\item  What are the two most commonly used attention functions?
\item  How is dot-product attention different from additive attention?
\item  How does the Transformer architecture improve machine translation tasks?
\item  What are the advantages of the Transformer architecture compared to recurrent or convolutional layers?
\item  What are the advantages of the Transformer model over other models such as RNNs and convolutional neural networks?
\item  What is the purpose of scaling dot-products in scaled dot-product attention?
\item  How does the Transformer reduce the number of operations required to relate signals from two arbitrary input or output positions compared to other models?
\item  How does the Transformer counteract the effect of reduced effective resolution due to averaging attention-weighted positions?
\item  How does the Transformer compare to RNN sequence-to-sequence models?
\item  How does the Transformer perform when trained on a limited amount of data?
\item  What is the Transformer and how does it compare to other sequence transduction models?
\item  How is the Transformer model superior in quality while being more parallelizable and requiring significantly less time to train?
\item  What is the Transformer architecture?
\item  How does the Transformer architecture use attention mechanisms to achieve superior results in machine translation tasks?
\item  What are positional encodings and how do they help the Transformer model make use of the order of the sequence?
\item  What is the Transformer model architecture and how does it differ from other models?
\item  How does the Transformer model compare in quality and training costs to other models from the literature?
\item  What is multi-head attention and how does it help the model?
\end{enumerate}
\end{document}